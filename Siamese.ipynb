{"cells":[{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2500,"status":"ok","timestamp":1738693940740,"user":{"displayName":"י ג","userId":"03873793388556586840"},"user_tz":-120},"id":"ar_jT30f1RtL","outputId":"cb55a250-8ddd-4467-8051-d11ac6b2ae92"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Successfully found project directory at: /content/drive/MyDrive/cv-22928-2025-a-project/train\n"]}],"source":["# [Cell 1] - Set up paths\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# !unzip \"/content/drive/MyDrive/cv-22928-2025-a-project.zip\" -d \"/content/\"\n","\n","BASE_PATH = \"/content/drive/MyDrive\"  # This will use the current directory\n","train_folder = \"cv-22928-2025-a-project/train\"\n","test_folder = os.path.join(BASE_PATH,  \"cv-22928-2025-a-project/test_images\")\n","project_folder = os.path.join(BASE_PATH, train_folder)\n","\n","if os.path.exists(project_folder):\n","    print(f\"Successfully found project directory at: {project_folder}\")\n","else:\n","    print(f\"ERROR: Could not find project directory at: {project_folder}\")\n","    print(f\"Current working directory: {os.getcwd()}\")\n","    print(f\"Please check if the path '{project_folder}' is correct\")"]},{"cell_type":"code","source":["\n","def test_model(test_csv, submission_path, estimate_fundamental_matrix):\n","\n","    # Verify test.csv exists\n","    if not os.path.exists(test_csv):\n","        print(f\"Error: test.csv not found at {test_csv}\")\n","        return\n","\n","    # Read test pairs\n","    try:\n","        with open(test_csv) as f:\n","            reader = csv.reader(f, delimiter=',')\n","            next(reader)  # Skip header\n","            test_samples = list(reader)\n","    except Exception as e:\n","        print(f\"Error reading test.csv: {e}\")\n","        return\n","\n","    # Initialize dictionary to store results\n","    F_dict = {}\n","\n","    # Process all test samples\n","    for row in tqdm(test_samples, desc='Processing test samples'):\n","        sample_id, batch_id, image_1_id, image_2_id = row\n","\n","        # Update image paths\n","        img1_path = os.path.join(src, 'test_images', batch_id, f'{image_1_id}.jpg')\n","        img2_path = os.path.join(src, 'test_images', batch_id, f'{image_2_id}.jpg')\n","\n","        # Process image pair\n","        F = estimate_fundamental_matrix(img1_path, img2_path)\n","        F_dict[sample_id] = F if F is not None else np.zeros((3, 3))\n","\n","\n","    # Create submission file\n","    with open(submission_path, 'w') as f:\n","        f.write('sample_id,fundamental_matrix\\n')\n","        for sample_id, F in F_dict.items():\n","            f.write(f'{sample_id},{flatten_matrix(F)}\\n')\n","\n","    print(f\"Submission saved to: {submission_path}\")"],"metadata":{"id":"jQARfwWJF5us","executionInfo":{"status":"ok","timestamp":1738692075259,"user_tz":-120,"elapsed":578,"user":{"displayName":"י ג","userId":"03873793388556586840"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["\n","!pip install tqdm\n","!pip install torch torchvision"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9dJTfJyG-ByS","executionInfo":{"status":"ok","timestamp":1738689762213,"user_tz":-120,"elapsed":114361,"user":{"displayName":"י ג","userId":"03873793388556586840"}},"outputId":"0294ae9f-4ff2-4318-a9b9-106f54cc45f3"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.20.1+cu124)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"]}]},{"cell_type":"code","execution_count":21,"metadata":{"id":"rFs-oRh_1Sg6","executionInfo":{"status":"ok","timestamp":1738693945752,"user_tz":-120,"elapsed":258,"user":{"displayName":"י ג","userId":"03873793388556586840"}}},"outputs":[],"source":["from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset, random_split\n","import torchvision.transforms as transforms\n","import cv2\n","import numpy as np\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","\n","class FundamentalMatrixNet(nn.Module):\n","    def __init__(self):\n","        super(FundamentalMatrixNet, self).__init__()\n","\n","        # Define the convolutional layers (shared between both images)\n","        self.model = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU(),\n","            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n","            nn.ReLU())\n","        self.fc1 = nn.Linear(256 * 32 * 32 * 32, 32)  # Assuming input image size of 128x128\n","        self.fc_confidence = nn.Linear(32, 1)  # Output is a scalar\n","        self.fc2 = nn.Linear(32, 3 * 3)  # Output is a 3x3 matrix\n","\n","    def forward(self, x1, x2):\n","        # Pass through the shared layers for both images\n","        x1 = self.model(x1)\n","        x2 = self.model(x2)\n","\n","        # Flatten the outputs from conv layers\n","        x1 = x1.view(x1.size(0), -1)\n","        x2 = x2.view(x2.size(0), -1)\n","\n","        # Combine features from both images\n","        x = torch.cat((x1, x2), dim=1)\n","\n","        # Fully connected layers\n","        x = self.fc1(x)\n","        x = torch.relu(x)\n","        F = self.fc2(x)\n","        confidence = self.fc_confidence(x)\n","        confidence = torch.heaviside(confidence, torch.tensor([0.5]))\n","        # Reshape the output to be a 3x3 matrix\n","        F = F.view(-1, 3, 3) * confidence\n","        return F"]},{"cell_type":"code","source":["\n","class ImagePairDataset(Dataset):\n","    def __init__(self, image_pairs, fundamental_matrices, transform=None):\n","        self.image_pairs = image_pairs\n","        self.fundamental_matrices = fundamental_matrices\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.image_pairs)\n","\n","    def __getitem__(self, idx):\n","        img1, img2 = self.image_pairs[idx]\n","        F = self.fundamental_matrices[idx]\n","\n","        img1 = cv2.imread(img1)\n","        img2 = cv2.imread(img2)\n","\n","        img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n","        img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n","\n","        img1 = cv2.resize(img1, (128, 128))\n","        img2 = cv2.resize(img2, (128, 128))\n","\n","        img1 = np.transpose(img1, (2, 0, 1))  # Change to CxHxW\n","        img2 = np.transpose(img2, (2, 0, 1))\n","\n","        img1 = torch.tensor(img1, dtype=torch.float32) / 255.0\n","        img2 = torch.tensor(img2, dtype=torch.float32) / 255.0\n","\n","        if self.transform:\n","            img1 = self.transform(img1)\n","            img2 = self.transform(img2)\n","\n","        F = torch.tensor(F, dtype=torch.float32)\n","        return img1, img2, F"],"metadata":{"id":"VY1fId3M16qr","executionInfo":{"status":"ok","timestamp":1738692091689,"user_tz":-120,"elapsed":313,"user":{"displayName":"י ג","userId":"03873793388556586840"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["from collections import namedtuple\n","import csv\n","\n","bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n","\n","sift_detector = cv2.SIFT_create(nfeatures=5000, contrastThreshold=-10000, edgeThreshold=-10000)\n","\n","def ComputeErrorForOneExample(q_gt, T_gt, q, T, scale, eps=1e-15):\n","    '''Compute the error metric for a single example.\n","\n","    The function returns two errors, over rotation and translation. These are combined at different thresholds by ComputeMaa in order to compute the mean Average Accuracy.'''\n","\n","    q_gt_norm = q_gt / (np.linalg.norm(q_gt) + eps)\n","    q_norm = q / (np.linalg.norm(q) + eps)\n","\n","    loss_q = np.maximum(eps, (1.0 - np.sum(q_norm * q_gt_norm)**2))\n","    err_q = np.arccos(1 - 2 * loss_q)\n","\n","    # Apply the scaling factor for this scene.\n","    T_gt_scaled = T_gt * scale\n","    T_scaled = T * np.linalg.norm(T_gt) * scale / (np.linalg.norm(T) + eps)\n","\n","    err_t = min(np.linalg.norm(T_gt_scaled - T_scaled), np.linalg.norm(T_gt_scaled + T_scaled))\n","\n","    return err_q * 180 / np.pi, err_t\n","\n","def ExtractSiftFeatures(image, detector, num_features):\n","    '''Compute SIFT features for a given image.'''\n","\n","    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n","    kp, desc = detector.detectAndCompute(gray, None)\n","    return kp[:num_features], desc[:num_features]\n","\n","def ArrayFromCvKps(kps):\n","    '''Convenience function to convert OpenCV keypoints into a simple numpy array.'''\n","\n","    return np.array([kp.pt for kp in kps])\n","\n","def get_inlier(path_img_1, path_img_2, inlier_mask, sift_detector=sift_detector):\n","    images_1 = cv2.cvtColor(cv2.imread(path_img_1), cv2.COLOR_BGR2RGB)\n","\n","    kp_dict_1, desc_dict_1 = ExtractSiftFeatures(images_1, sift_detector, 2000)\n","\n","    images_2 = cv2.cvtColor(cv2.imread(path_img_2), cv2.COLOR_BGR2RGB)\n","    kp_dict_2, desc_dict_2 = ExtractSiftFeatures(images_2, sift_detector, 2000)\n","\n","    cv_matches = bf.match(desc_dict_1, desc_dict_2)\n","    matches = np.array([[m.queryIdx, m.trainIdx] for m in cv_matches])\n","\n","    matches_after_ransac = np.array([match for match, is_inlier in zip(matches, inlier_mask) if is_inlier])\n","    inlier_kp_1 = ArrayFromCvKps([kp_dict_1[m[0]] for m in matches_after_ransac])\n","    inlier_kp_2 = ArrayFromCvKps([kp_dict_2[m[1]] for m in matches_after_ransac])\n","    return inlier_kp_1, inlier_kp_2\n","\n","def QuaternionFromMatrix(matrix):\n","    '''Transform a rotation matrix into a quaternion.'''\n","\n","    M = np.array(matrix, dtype=np.float64, copy=False)[:4, :4]\n","    m00 = M[0, 0]\n","    m01 = M[0, 1]\n","    m02 = M[0, 2]\n","    m10 = M[1, 0]\n","    m11 = M[1, 1]\n","    m12 = M[1, 2]\n","    m20 = M[2, 0]\n","    m21 = M[2, 1]\n","    m22 = M[2, 2]\n","\n","    K = np.array([[m00 - m11 - m22, 0.0, 0.0, 0.0],\n","              [m01 + m10, m11 - m00 - m22, 0.0, 0.0],\n","              [m02 + m20, m12 + m21, m22 - m00 - m11, 0.0],\n","              [m21 - m12, m02 - m20, m10 - m01, m00 + m11 + m22]])\n","    K /= 3.0\n","\n","    # The quaternion is the eigenvector of K that corresponds to the largest eigenvalue.\n","    w, V = np.linalg.eigh(K)\n","    q = V[[3, 0, 1, 2], np.argmax(w)]\n","\n","    if q[0] < 0:\n","        np.negative(q, q)\n","\n","    return q\n","\n","def calc_accuracy(calib_dict_img_1, calib_dict_img_2, path_img_1, path_img_2, inlier_mask,\n","                   scale, F, sift_detector=sift_detector, eps=1e-15):\n","    inlier_kp_1, inlier_kp_2 = get_inlier(path_img_1, path_img_2, inlier_mask, sift_detector)\n","\n","    E, R, T = ComputeEssentialMatrix(F, calib_dict_img_1.K, calib_dict_img_2.K, inlier_kp_1, inlier_kp_2)\n","    q = QuaternionFromMatrix(R)\n","    T = T.flatten()\n","\n","    # Get the ground truth relative pose difference for this pair of images.\n","    R1_gt, T1_gt = calib_dict_img_1.R, calib_dict_img_1.T.reshape((3, 1))\n","    R2_gt, T2_gt = calib_dict_img_2.R, calib_dict_img_2.T.reshape((3, 1))\n","    dR_gt = np.dot(R2_gt, R1_gt.T)\n","    dT_gt = (T2_gt - np.dot(dR_gt, T1_gt)).flatten()\n","    q_gt = QuaternionFromMatrix(dR_gt)\n","    q_gt = q_gt / (np.linalg.norm(q_gt) + eps)\n","\n","    # Given ground truth and prediction, compute the error for the example above.\n","    err_q_curr, err_t_curr = ComputeErrorForOneExample(q_gt, dT_gt, q, T, scale=scale)\n","    return err_q_curr, err_t_curr\n","\n","def NormalizeKeypoints(keypoints, K):\n","    C_x = K[0, 2]\n","    C_y = K[1, 2]\n","    f_x = K[0, 0]\n","    f_y = K[1, 1]\n","    keypoints = (keypoints - np.array([[C_x, C_y]])) / np.array([[f_x, f_y]])\n","    return keypoints\n","\n","\n","def ComputeEssentialMatrix(F, K1, K2, kp1, kp2):\n","    '''Compute the Essential matrix from the Fundamental matrix, given the calibration matrices. Note that we ask participants to estimate F, i.e., without relying on known intrinsics.'''\n","\n","    # Warning! Old versions of OpenCV's RANSAC could return multiple F matrices, encoded as a single matrix size 6x3 or 9x3, rather than 3x3.\n","    # We do not account for this here, as the modern RANSACs do not do this:\n","    # https://opencv.org/evaluating-opencvs-new-ransacs\n","    assert F.shape[0] == 3, 'Malformed F?'\n","\n","    # Use OpenCV's recoverPose to solve the cheirality check:\n","    # https://docs.opencv.org/4.5.4/d9/d0c/group__calib3d.html#gadb7d2dfcc184c1d2f496d8639f4371c0\n","    E = np.matmul(np.matmul(K2.T, F), K1).astype(np.float64)\n","\n","    kp1n = NormalizeKeypoints(kp1, K1)\n","    kp2n = NormalizeKeypoints(kp2, K2)\n","    num_inliers, R, T, mask = cv2.recoverPose(E, kp1n, kp2n)\n","\n","    return E, R, T\n","\n","def LoadCalibration(filename):\n","    Gt = namedtuple('Gt', ['K', 'R', 'T'])\n","\n","    '''Load calibration data (ground truth) from the csv file.'''\n","\n","    calib_dict = {}\n","    with open(filename, 'r') as f:\n","        reader = csv.reader(f, delimiter=',')\n","        for i, row in enumerate(reader):\n","            # Skip header.\n","            if i == 0:\n","                continue\n","\n","            camera_id = row[1]\n","            K = np.array([float(v) for v in row[2].split(' ')]).reshape([3, 3])\n","            R = np.array([float(v) for v in row[3].split(' ')]).reshape([3, 3])\n","            T = np.array([float(v) for v in row[4].split(' ')])\n","            calib_dict[camera_id] = Gt(K=K, R=R, T=T)\n","\n","    return calib_dict\n","\n","def ComputeMaa(err_q, err_t,thresholds_q = np.linspace(1, 10, 10), thresholds_t = np.geomspace(0.2, 5, 10)):\n","    '''Compute the mean Average Accuracy at different tresholds, for one scene.'''\n","\n","    assert len(err_q) == len(err_t)\n","\n","    acc, acc_q, acc_t = [], [], []\n","    for th_q, th_t in zip(thresholds_q, thresholds_t):\n","        acc += [(np.bitwise_and(np.array(err_q) < th_q, np.array(err_t) < th_t)).sum() / len(err_q)]\n","        acc_q += [(np.array(err_q) < th_q).sum() / len(err_q)]\n","        acc_t += [(np.array(err_t) < th_t).sum() / len(err_t)]\n","    return np.mean(acc), np.array(acc), np.array(acc_q), np.array(acc_t)"],"metadata":{"id":"OZ9TEuw3_Tbp","executionInfo":{"status":"ok","timestamp":1738692095720,"user_tz":-120,"elapsed":1044,"user":{"displayName":"י ג","userId":"03873793388556586840"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["import glob\n","import pandas as pd\n","import csv\n","\n","class_folders = glob.glob(project_folder + \"/*/\")\n","print(class_folders)\n","\n","\n","scaling_dict = {}\n","with open(f'{project_folder}/scaling_factors.csv') as f:\n","    reader = csv.reader(f, delimiter=',')\n","    for i, row in enumerate(reader):\n","        # Skip header.\n","        if i == 0:\n","            continue\n","        scaling_dict[row[1]] = float(row[2])\n","\n","dataframes = []\n","calib_dict = {}\n","# Loop through the CSV files and read them into DataFrames\n","for folder in class_folders:\n","    calib_dict[folder] = LoadCalibration(f'{folder}/calibration.csv')\n","\n","    df = pd.read_csv(folder + \"/pair_covisibility.csv\", index_col=False)\n","    df[\"img1_path\"] = folder + \"images/\" + df[\"im1\"] + \".jpg\"\n","    df[\"img2_path\"] = folder + \"images/\" + df[\"im1\"] + \".jpg\"\n","    df[\"folder\"] = folder\n","    df[\"scale\"] = scaling_dict[folder.split('/')[-2]]\n","    dataframes.append(df)\n","\n","combined_df = pd.concat(dataframes, ignore_index=True)\n","combined_df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TJXwz4VhQa4v","executionInfo":{"status":"ok","timestamp":1738692101164,"user_tz":-120,"elapsed":1075,"user":{"displayName":"י ג","userId":"03873793388556586840"}},"outputId":"bb4d2332-0d98-4496-8e5a-0a88a4b1eb21"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["['/content/drive/MyDrive/cv-22928-2025-a-project/train/trevi_fountain/', '/content/drive/MyDrive/cv-22928-2025-a-project/train/sacre_coeur/', '/content/drive/MyDrive/cv-22928-2025-a-project/train/notre_dame_front_facade/', '/content/drive/MyDrive/cv-22928-2025-a-project/train/temple_nara_japan/', '/content/drive/MyDrive/cv-22928-2025-a-project/train/taj_mahal/', '/content/drive/MyDrive/cv-22928-2025-a-project/train/sagrada_familia/', '/content/drive/MyDrive/cv-22928-2025-a-project/train/lincoln_memorial_statue/', '/content/drive/MyDrive/cv-22928-2025-a-project/train/colosseum_exterior/', '/content/drive/MyDrive/cv-22928-2025-a-project/train/pantheon_exterior/', '/content/drive/MyDrive/cv-22928-2025-a-project/train/brandenburg_gate/', '/content/drive/MyDrive/cv-22928-2025-a-project/train/british_museum/', '/content/drive/MyDrive/cv-22928-2025-a-project/train/buckingham_palace/']\n"]},{"output_type":"execute_result","data":{"text/plain":["(84578, 10)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["df = combined_df.loc[combined_df.covisibility>0.1, :]\n","df = df.copy()\n","df.loc[:, \"F_romatch\"] = df.loc[:, \"fundamental_matrix\"]\n","df.shape\n","df = df.iloc[[1]]"],"metadata":{"id":"-aegA3Os_y89","executionInfo":{"status":"ok","timestamp":1738692107343,"user_tz":-120,"elapsed":298,"user":{"displayName":"י ג","userId":"03873793388556586840"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# [Cell 11] - Helper function for matrix flattening\n","def flatten_matrix(M, num_digits=8):\n","    \"\"\"Convert matrix to string format for submission.\"\"\"\n","    return ' '.join([f'{v:.{num_digits}e}' for v in M.flatten()])\n","\n","def unflatten_matrix(flattened_str):\n","    \"\"\"Convert a flattened string back into a 3x3 matrix.\"\"\"\n","    # Split the flattened string into a list of values\n","    values = list(map(float, flattened_str.split()))\n","\n","    # Ensure the number of values is 9 (for a 3x3 matrix)\n","    if len(values) != 9:\n","        raise ValueError(\"The flattened string must contain exactly 9 values.\")\n","\n","    # Reshape the list of values into a 3x3 matrix\n","    return np.array(values).reshape(3, 3)"],"metadata":{"id":"VAmq21x-6x8P","executionInfo":{"status":"ok","timestamp":1738692109986,"user_tz":-120,"elapsed":324,"user":{"displayName":"י ג","userId":"03873793388556586840"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Example data (replace with real dataset)\n","\n","image_pairs = [x for x in zip(df[\"img1_path\"], df[\"img2_path\"])] # Image pairs (replace with your dataset)\n","fundamental_matrices = [unflatten_matrix(x) for x in df[\"fundamental_matrix\"]]# Random fundamental matrices (replace with ground truth)\n","\n","# Create dataset and dataloader\n","dataset = ImagePairDataset(image_pairs, fundamental_matrices)\n","dataloader = DataLoader(dataset, batch_size=10, shuffle=True)"],"metadata":{"id":"txwyVZ5g3DFk","executionInfo":{"status":"ok","timestamp":1738692114634,"user_tz":-120,"elapsed":373,"user":{"displayName":"י ג","userId":"03873793388556586840"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["\n","\n","# Initialize model, loss function, and optimizer\n","model = FundamentalMatrixNet()\n","criterion = nn.L1Loss()  # Mean Squared Error loss\n","optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n","\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","trainable_params"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m2y44lmHE7WZ","executionInfo":{"status":"ok","timestamp":1738693990315,"user_tz":-120,"elapsed":3815,"user":{"displayName":"י ג","userId":"03873793388556586840"}},"outputId":"3ab99c13-76cd-40b8-91cf-c8aa2d2c1284"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["268806634"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["\n","\n","\n","# Training loop\n","for epoch in range(2):\n","    epoch_loss = 0\n","    for img1, img2, F_gt in tqdm(dataloader):\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        F_pred = model.forward(img1, img2)\n","\n","        # Compute loss\n","        loss = criterion(F_pred, F_gt)\n","\n","        # Backward pass and optimize\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(dataloader)}\")\n","torch.save(model, f\"{project_folder}/model.pth\")"],"metadata":{"id":"nWZZKCg72JG1","colab":{"base_uri":"https://localhost:8080/"},"outputId":"9a2b837e-941e-41c4-969a-1ab6d348f42f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/1 [00:00<?, ?it/s]"]}]},{"cell_type":"code","source":["with torch.no_grad():\n","        val_loss = 0.0\n","        for left_image, right_image, F in val_loader:\n","            F_pred = model(left_image, right_image)\n","            loss = criterion(F_pred, F)\n","            val_loss += loss.item()\n","        print(f\"Validation Loss: {val_loss / len(val_loader)}\")"],"metadata":{"id":"RkrOf26PJbC7"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMDwb26QySEB+nw6BC5nQdI"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}